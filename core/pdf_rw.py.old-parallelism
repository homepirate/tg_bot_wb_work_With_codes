import os
import time
from dataclasses import dataclass
from typing import Optional, Tuple

import pdfplumber
from PyPDF2 import PdfReader, PdfWriter
from sqlalchemy.ext.asyncio import AsyncSession

from config import config
from services.printed_codes import register_code_if_new, bulk_register_codes, get_all_codes
from .patterns import *
import asyncio

@dataclass(frozen=True)
class CutResult:
    head_path: Optional[Path]
    shortage: int


# üîß helpers (–æ—Ñ—Ñ–ª–æ–∞–¥ —Å–∏–Ω—Ö—Ä–æ–Ω—â–∏–Ω—ã –≤ –ø–æ—Ç–æ–∫)
async def _to_thread(func, *args, **kwargs):
    return await asyncio.to_thread(func, *args, **kwargs)


def _write_pdf(writer: PdfWriter, out_path: Path) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "wb") as f:
        writer.write(f)

def _replace_file(tmp_path: Path, target: Path) -> None:
    os.replace(tmp_path, target)

def _strip_all_ws(s: str) -> str:
    return re.sub(r"\s+", "", s).lower()

def _ascii_prefix(line: str) -> Optional[str]:
    m = RE_ASCII_PREFIX.match(line)
    return m.group(1) if m else None


def _extract_code_from_text(text: str) -> Optional[str]:
    """
    –ò—â–µ–º GS1: (01)<14 —Ü–∏—Ñ—Ä>(21)<ASCII-serial> (—Å–æ/–±–µ–∑ —Å–∫–æ–±–æ–∫).
    –°–µ—Ä–∏–π–Ω–∏–∫ –º–æ–∂–µ—Ç –∏–¥—Ç–∏ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ (21) –∏–ª–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö ‚Äî –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å –Ω–∞—á–∞–ª–∞.
    –ü–æ—Å–ª–µ —Å–±–æ—Ä–∫–∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É 27..31.
    """
    if not text:
        return None

    # 0) –í—Å—ë –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ —Å–æ —Å–∫–æ–±–∫–∞–º–∏
    m_one = RE_GS1_PAREN_ONELINE.search(text)
    if m_one:
        candidate = re.sub(r"\s+", "", m_one.group(0))
        return candidate if 27 <= len(candidate) <= 35 else None

    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    if not lines:
        return None

    def pack(head: str, tail: str) -> Optional[str]:
        s = re.sub(r"\s+", "", head) + re.sub(r"\s+", "", tail)
        return s if 27 <= len(s) <= 31 else None

    LOOKAHEAD = 4  # —Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –¥–∞–ª—å—à–µ —Å–º–æ—Ç—Ä–∏–º

    # 1) –°–æ —Å–∫–æ–±–∫–∞–º–∏: "(01)‚Ä¶..(21)" –≤ —Å—Ç—Ä–æ–∫–µ i, —Å–µ—Ä–∏–π–Ω–∏–∫ –Ω–∞ —ç—Ç–æ–π –∂–µ –∏–ª–∏ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö (–≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ)
    head_pat = re.compile(r"\(\s*01\s*\)\s*\d{14}\s*\(\s*21\s*\)")  # —Ç–æ–ª—å–∫–æ "–≥–æ–ª–æ–≤—É"
    for i, ln in enumerate(lines):
        mh = head_pat.search(ln)
        if not mh:
            continue
        head = ln[:mh.end()]
        tail_same = ln[mh.end():]

        # —Å–µ—Ä–∏–π–Ω–∏–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –≥–¥–µ —É–≥–æ–¥–Ω–æ –≤ —Ö–≤–æ—Å—Ç–µ —Å—Ç—Ä–æ–∫–∏ (–ù–ï —Ç–æ–ª—å–∫–æ —Å –Ω–∞—á–∞–ª–∞)
        m_same = RE_ASCII_ANY.search(tail_same)
        if m_same:
            cand = pack(head, m_same.group(0))
            if cand:
                return cand

        # –ª–∏–±–æ –Ω–∞ –æ–¥–Ω–æ–π –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫ ‚Äî —Ç–æ–∂–µ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å –Ω–∞—á–∞–ª–∞
        for j in range(i + 1, min(i + 1 + LOOKAHEAD, len(lines))):
            m_next = RE_ASCII_ANY.search(lines[j])
            if m_next:
                cand = pack(head, m_next.group(0))
                if cand:
                    return cand

        # –ø–æ–ø—Ä–æ–±—É–µ–º ¬´—Å–∫–ª–µ–∏—Ç—å¬ª —Ö–≤–æ—Å—Ç –∏ –ø–∞—Ä—É —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫ –Ω–∞ —Å–ª—É—á–∞–π —Ä–∞–∑—Ä—ã–≤–æ–≤
        glued = tail_same + " " + " ".join(lines[i + 1:min(i + 1 + LOOKAHEAD, len(lines))])
        m_glued = RE_ASCII_ANY.search(glued)
        if m_glued:
            cand = pack(head, m_glued.group(0))
            if cand:
                return cand

    # 2) –ë–µ–∑ —Å–∫–æ–±–æ–∫: ¬´01\d{14}21¬ª –∫–∞–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∞, —Å–µ—Ä–∏–π–Ω–∏–∫ –¥–∞–ª–µ–µ (–≥–¥–µ —É–≥–æ–¥–Ω–æ)
    noparen_head_any = re.compile(r"01\s*\d{14}\s*21")
    for i, ln in enumerate(lines):
        mh = noparen_head_any.search(ln)
        if not mh:
            continue
        head = ln[mh.start():mh.end()]
        tail_same = ln[mh.end():]

        m_same = RE_ASCII_ANY.search(tail_same)
        if m_same:
            cand = pack(head, m_same.group(0))
            if cand:
                return cand

        for j in range(i + 1, min(i + 1 + LOOKAHEAD, len(lines))):
            m_next = RE_ASCII_ANY.search(lines[j])
            if m_next:
                cand = pack(head, m_next.group(0))
                if cand:
                    return cand

        glued = tail_same + " " + " ".join(lines[i + 1:min(i + 1 + LOOKAHEAD, len(lines))])
        m_glued = RE_ASCII_ANY.search(glued)
        if m_glued:
            cand = pack(head, m_glued.group(0))
            if cand:
                return cand

    return None


def read_pdf(file_path: str | Path) -> str:
    path = Path(file_path)
    parts: list[str] = []
    try:
        with pdfplumber.open(str(path)) as pdf:
            for p in pdf.pages:
                t = p.extract_text()
                if t:
                    parts.append(t.strip())
    except FileNotFoundError:
        print(f"[read_pdf] not found: {path}")
        return ""
    except Exception as e:
        print(f"[read_pdf] failed {path}: {e}")
        return ""
    return "\n".join(parts)


# ---- –ø–æ–∏—Å–∫ PDF –ø–æ (–∞—Ä—Ç–∏–∫—É–ª, —Ä–∞–∑–º–µ—Ä)
def _compile_size_token(size_raw: str) -> re.Pattern:
    """
    –ñ—ë—Å—Ç–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∞ –Ω–µ –ª—é–±–æ–≥–æ).
    - –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–∏—Ä–µ –∫ '-'
    - –¥–æ–ø—É—Å–∫–∞–µ–º '-', '‚Äì', '/', –º–µ–∂–¥—É —á–∏—Å–ª–∞–º–∏
    - –≥—Ä–∞–Ω–∏—Ü—ã —Ç–æ–∫–µ–Ω–∞ (–Ω–µ –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã —Å–ª–µ–≤–∞/—Å–ø—Ä–∞–≤–∞)
    """
    s = re.sub(r"\s+", "", str(size_raw)).upper()
    s = s.replace("‚Äì", "-").replace("‚Äî", "-")
    if re.fullmatch(r"[2-5]?(?:XS|S|M|L|XL|XXL|XXXL)", s):
        return re.compile(rf"(?<![A-Z0-9]){re.escape(s)}(?![A-Z0-9])", re.IGNORECASE | re.MULTILINE)
    token = re.escape(s).replace(r"\-", r"[‚Äì\-\/]")
    return re.compile(rf"(?<!\w){token}(?!\w)", re.IGNORECASE | re.MULTILINE)

def find_pdfs_by_article_size_all(article: str, size: str) -> list[Path]:
    results: list[Path] = []
    if not article or not size:
        return results

    a_no_ws = _strip_all_ws(str(article))
    size_regex = _compile_size_token(size)

    for pdf_file in PDF_DIR.glob("*.pdf"):
        try:
            raw_text = read_pdf(pdf_file)
        except Exception as e:
            print(e)
            continue

        # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–∏—Ä–µ –≤ —Ç–µ–∫—Å—Ç–µ –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–∞
        raw_text_norm = raw_text.replace("‚Äì", "-").replace("‚Äî", "-")

        if a_no_ws not in _strip_all_ws(raw_text):
            continue

        # —Ä–∞–∑–º–µ—Ä ‚Äî –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É
        if size_regex.search(raw_text_norm):
            results.append(pdf_file)

    results.sort(key=lambda p: p.name.lower())
    return results

def _build_tail_writer(reader: PdfReader, total: int, keep_indexes: set[int]) -> PdfWriter:
    w = PdfWriter()
    for i in range(total):
        if i in keep_indexes:
            w.add_page(reader.pages[i])
    return w

def _extract_page_code(pl_pdf, page_index: int) -> Optional[str]:
    txt = pl_pdf.pages[page_index].extract_text(x_tolerance=1.0, y_tolerance=1.0) or ""
    return _extract_code_from_text(txt)

async def cut_first_n_pages_unique_checkonly(
    src_pdf: Path | str,
    n: int,
    used_codes: set[str],
    staged_codes: set[str],
) -> Tuple[Optional[Path], int, list[str]]:
    """
    –í—ã—Ä–µ–∑–∞–µ—Ç –ø–µ—Ä–≤—ã–µ n —Å—Ç—Ä–∞–Ω–∏—Ü —Å –ù–û–í–´–ú–ò –∫–æ–¥–∞–º–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ used_codes ‚à™ staged_codes.
    –ë–∞–∑—É –Ω–µ —Ç—Ä–æ–≥–∞–µ—Ç. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–ø—É—Ç—å_–∫_—à–∞–ø–∫–µ|None, –Ω–µ—Ö–≤–∞—Ç–∫–∞, —Å–ø–∏—Å–æ–∫_–≤—ã–±—Ä–∞–Ω–Ω—ã—Ö_–∫–æ–¥–æ–≤).
    –í–ù–ò–ú–ê–ù–ò–ï: –∏—Å—Ö–æ–¥–Ω—ã–π PDF –≤—Å—ë —Ç–∞–∫ –∂–µ —É—Ä–µ–∑–∞–µ—Ç—Å—è (–∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ).
    """
    src = Path(src_pdf)
    if n <= 0:
        return None, 0, []

    tmp_dir = src.parent / "tmp"; tmp_dir.mkdir(parents=True, exist_ok=True)
    try:
        reader = await _to_thread(PdfReader, str(src))
    except Exception:
        return None, n, []

    total_pages = len(reader.pages)
    to_delete: set[int] = set()
    head_writer = PdfWriter()
    unique_taken = 0
    picked_codes: list[str] = []

    def _read_texts():
        with pdfplumber.open(str(src)) as pl:
            return [pl.pages[i].extract_text(x_tolerance=1.0, y_tolerance=1.0) or "" for i in range(len(pl.pages))]

    try:
        texts = await _to_thread(_read_texts)
    except Exception:
        return None, n, []

    for i in range(total_pages):
        if unique_taken >= n:
            break
        code = _extract_code_from_text(texts[i])
        if not code:
            continue
        if code in used_codes or code in staged_codes:
            # —É–∂–µ –≤–∏–¥–µ–ª–∏ ‚Äî –≤—ã–∫–∏–¥—ã–≤–∞–µ–º —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            to_delete.add(i)
            continue

        # –Ω–æ–≤—ã–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏
        staged_codes.add(code)
        picked_codes.append(code)
        head_writer.add_page(reader.pages[i])
        to_delete.add(i)
        unique_taken += 1

    if unique_taken == 0:
        if to_delete:
            keep = set(range(total_pages)) - to_delete
            tail_writer = _build_tail_writer(reader, total_pages, keep)
            if len(tail_writer.pages) > 0:
                tail_tmp = tmp_dir / f"{src.stem}__tail_tmp.pdf"
                await _to_thread(_write_pdf, tail_writer, tail_tmp)
                await _to_thread(_replace_file, tail_tmp, src)
            else:
                try:
                    await _to_thread(src.unlink, True)
                except Exception:
                    pass
        return None, n, []

    head_out = tmp_dir / f"{src.stem}__head_{unique_taken}.pdf"
    await _to_thread(_write_pdf, head_writer, head_out)

    keep = set(range(total_pages)) - to_delete
    if keep:
        tail_writer = _build_tail_writer(reader, total_pages, keep)
        tail_tmp = tmp_dir / f"{src.stem}__tail_tmp.pdf"
        await _to_thread(_write_pdf, tail_writer, tail_tmp)
        await _to_thread(_replace_file, tail_tmp, src)
    else:
        try:
            await _to_thread(src.unlink, True)
        except Exception:
            pass

    return head_out, max(0, n - unique_taken), picked_codes


def merge_pdfs(pdf_paths: list[Path | str], output_path: Path | str) -> Path:
    writer = PdfWriter()
    for p in pdf_paths:
        pth = Path(p)
        if not pth.exists():
            continue
        reader = PdfReader(str(pth))
        for page in reader.pages:
            writer.add_page(page)
    out = Path(output_path)
    out.parent.mkdir(parents=True, exist_ok=True)
    with open(out, "wb") as f:
        writer.write(f)
    return out

def _normalize_columns(df) -> tuple[int, int, int]:
    required = {"–∞—Ä—Ç–∏–∫—É–ª", "—Ä–∞–∑–º–µ—Ä", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ"}
    cols_norm = [str(c).strip().lower() for c in df.columns]
    colset = set(cols_norm)
    if not required.issubset(colset):
        missing = required - colset
        raise ValueError(f"–í df –Ω–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {', '.join(sorted(missing))}")
    return cols_norm.index("–∞—Ä—Ç–∏–∫—É–ª"), cols_norm.index("—Ä–∞–∑–º–µ—Ä"), cols_norm.index("–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ")

def _append_shortage(shortages: list[str], article: str, size: str, amount: int) -> None:
    shortages.append(f"{article} - —Ä–∞–∑–º–µ—Ä: {size}, –Ω–µ —Ö–≤–∞—Ç–∏–ª–æ: {amount}")



async def _process_order_row(
    row_no: int,
    row,
    idx_article: int,
    idx_size: int,
    idx_qty: int,
    used_codes: set[str],
) -> tuple[int, list[Path], set[str], list[str]]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –∑–∞–∫–∞–∑–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      (row_no, parts, staged_codes_local, shortages_local)
    """
    parts: list[Path] = []
    staged_local: set[str] = set()
    shortages_local: list[str] = []

    article = str(row.iloc[idx_article]).strip()
    size    = str(row.iloc[idx_size]).strip()

    try:
        qty = int(row.iloc[idx_qty])
    except Exception:
        return row_no, parts, staged_local, shortages_local

    if qty <= 0:
        return row_no, parts, staged_local, shortages_local

    try:
        pdf_paths = await _to_thread(find_pdfs_by_article_size_all, article, size)
    except Exception:
        pdf_paths = []

    if not pdf_paths:
        _append_shortage(shortages_local, article, size, qty)
        return row_no, parts, staged_local, shortages_local

    remaining = qty

    for src_pdf_path in pdf_paths:
        if remaining <= 0:
            break

        print(f"Check {src_pdf_path}")

        part_path, shortage, picked_codes = await cut_first_n_pages_unique_checkonly(
            src_pdf_path,
            remaining,
            used_codes,
            staged_local,   # <-- –ª–æ–∫–∞–ª—å–Ω—ã–π set, –±–µ–∑–æ–ø–∞—Å–Ω–æ
        )

        took_now = max(0, remaining - shortage)

        if took_now > 0 and part_path is not None:
            # –ø—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            try:
                rr = await _to_thread(PdfReader, str(part_path))
                if len(rr.pages) > 0:
                    parts.append(Path(part_path))
                else:
                    try:
                        await _to_thread(Path(part_path).unlink, True)
                    except Exception:
                        pass
            except Exception:
                pass

        remaining -= took_now

    if remaining > 0:
        _append_shortage(shortages_local, article, size, remaining)

    return row_no, parts, staged_local, shortages_local


async def build_pdf_from_dataframe(
    df,
    output_path: Path | str | None = None,
) -> tuple[Optional[Path], Optional[str]]:
    idx_article, idx_size, idx_qty = _normalize_columns(df)

    PARALLELISM = 30
    sem = asyncio.Semaphore(PARALLELISM)

    rows = list(df.iterrows())
    total = len(rows)

    # –ø—Ä–æ–≥—Ä–µ—Å—Å
    done = 0
    inflight = 0
    lock = asyncio.Lock()
    t0 = time.time()

    def _fmt_eta(done_: int) -> str:
        if done_ <= 0:
            return "ETA: ?"
        elapsed = time.time() - t0
        per = elapsed / done_
        eta = per * (total - done_)
        return f"ETA: {int(eta)}s"

    async with config.AsyncSessionLocal() as session:
        async with session.begin():
            used_codes = await get_all_codes(session)

            async def _run_one(row_no: int, row_obj):
                nonlocal inflight, done

                async with sem:
                    # START log
                    async with lock:
                        inflight += 1
                        print(f"[{row_no+1}/{total}] START  inflight={inflight}")

                    try:
                        res = await _process_order_row(
                            row_no=row_no,
                            row=row_obj,
                            idx_article=idx_article,
                            idx_size=idx_size,
                            idx_qty=idx_qty,
                            used_codes=used_codes,
                        )
                        return res
                    finally:
                        # END log
                        async with lock:
                            inflight -= 1
                            done += 1
                            print(f"[{done}/{total}] DONE   inflight={inflight}  {_fmt_eta(done)}")

            tasks = [
                _run_one(i, row)
                for i, (_, row) in enumerate(rows)
            ]

            results = await asyncio.gather(*tasks)
            results.sort(key=lambda x: x[0])

            cut_parts: list[Path] = []
            staged_codes_all: set[str] = set()
            shortages: list[str] = []

            for _, parts, staged_local, shortages_local in results:
                if parts:
                    cut_parts.extend(parts)
                if staged_local:
                    staged_codes_all.update(staged_local)
                if shortages_local:
                    shortages.extend(shortages_local)

            if not cut_parts:
                return None, ("\n".join(shortages) if shortages else None)

            try:
                result_path = await _to_thread(
                    merge_pdfs,
                    cut_parts,
                    output_path or (PDF_DIR / "result.pdf"),
                )
            except Exception:
                return None, ("\n".join(shortages) if shortages else None)

            try:
                await bulk_register_codes(session, staged_codes_all)
            except Exception as e:
                try:
                    Path(result_path).unlink(missing_ok=True)
                except Exception:
                    pass
                raise e

    for p in cut_parts:
        try:
            await _to_thread(Path(p).unlink, True)
        except Exception:
            pass

    report = "\n".join(shortages) if shortages else None
    return Path(result_path), report