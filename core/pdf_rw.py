import os
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, Optional, Tuple

import pdfplumber
from PyPDF2 import PdfReader, PdfWriter
from sqlalchemy.ext.asyncio import AsyncSession

from config import config
from services.printed_codes import register_code_if_new
from .patterns import *
import asyncio

@dataclass(frozen=True)
class CutResult:
    head_path: Optional[Path]
    shortage: int


# üîß helpers (–æ—Ñ—Ñ–ª–æ–∞–¥ —Å–∏–Ω—Ö—Ä–æ–Ω—â–∏–Ω—ã –≤ –ø–æ—Ç–æ–∫)
async def _to_thread(func, *args, **kwargs):
    return await asyncio.to_thread(func, *args, **kwargs)


def _write_pdf(writer: PdfWriter, out_path: Path) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "wb") as f:
        writer.write(f)

def _replace_file(tmp_path: Path, target: Path) -> None:
    os.replace(tmp_path, target)

def _strip_all_ws(s: str) -> str:
    return re.sub(r"\s+", "", s).lower()

def _ascii_prefix(line: str) -> Optional[str]:
    m = RE_ASCII_PREFIX.match(line)
    return m.group(1) if m else None


def _extract_code_from_text(text: str) -> Optional[str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç GS1-–∫–æ–¥ —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
      (01)<14 —Ü–∏—Ñ—Ä>(21)<ASCII-serial>
    –î–æ–ø—É—Å—Ç–∏–º –ø–µ—Ä–µ–Ω–æ—Å: —Å–µ—Ä–∏–∞–ª –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–æ–∫–µ.
    –õ—é–±—ã–µ –Ω–µ-ASCII (–Ω–∞–ø—Ä. '–≥–æ–ª—É–±–æ–π') –ø–æ—Å–ª–µ —Å–µ—Ä–∏–π–Ω–∏–∫–∞ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è.
    """
    if not text:
        return None

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ —á–∏—Å—Ç–∏–º
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    if not lines:
        return None

    # --- –í–ê–†–ò–ê–ù–¢ A: –≤—Å—ë –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ ---
    # –†–µ–≥—ç–∫—Å –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Å–µ—Ä–∏–∞–ª —Ç–æ–ª—å–∫–æ –ø–µ—á–∞—Ç–Ω—ã–º ASCII, —Ç–∞–∫ —á—Ç–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –Ω–µ –ø–æ–ø–∞–¥—ë—Ç.
    m_one = RE_GS1_PAREN_ONELINE.search(text)
    if m_one:
        return re.sub(r"\s+", "", m_one.group(0))

    # –•–µ–ª–ø–µ—Ä: —Å–æ–±—Ä–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é "–≥–æ–ª–æ–≤—É" –∏ —Å–µ—Ä–∏–∞–ª
    def _pack(head_line: str, serial_ascii: str) -> str:
        head = re.sub(r"\s+", "", head_line)
        tail = re.sub(r"\s+", "", serial_ascii)
        return head + tail

    # --- –í–ê–†–ò–ê–ù–¢ B: (01)‚Ä¶(21) –≤ —Å—Ç—Ä–æ–∫–µ i, —Å–µ—Ä–∏–∞–ª –º–æ–∂–µ—Ç –±—ã—Ç—å:
    #   - —Å—Ä–∞–∑—É –∑–∞ (21) –Ω–∞ —Ç–æ–π –∂–µ —Å—Ç—Ä–æ–∫–µ (–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π ASCII),
    #   - –ª–∏–±–æ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π/—á–µ—Ä–µ–∑ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫–µ –∫–∞–∫ ASCII-–ø—Ä–µ—Ñ–∏–∫—Å.
    for i, ln in enumerate(lines):
        if "(01)" in ln and "(21)" in ln:
            # –≤—ã—Ü–µ–ø–ª—è–µ–º ¬´–≥–æ–ª–æ–≤—É¬ª –≤–ø–ª–æ—Ç—å –¥–æ (–≤–∫–ª—é—á–∞—è) (21)
            m_head = re.search(r"\(\s*01\s*\)\s*\d{14}\s*\(\s*21\s*\)", ln)
            if not m_head:
                # –Ω–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –≥–æ–ª–æ–≤—ã ‚Äî –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∫–µ–π—Å
                continue

            head_line = ln[: m_head.end()]  # –¥–æ –∫–æ–Ω—Ü–∞ '(21)'
            tail_same = ln[m_head.end():]  # –≤—Å—ë, —á—Ç–æ –ø–æ—Å–ª–µ '(21)' –≤ —ç—Ç–æ–π —Å—Ç—Ä–æ–∫–µ

            # 1) —Å–µ—Ä–∏–∞–ª –Ω–∞ —Ç–æ–π –∂–µ —Å—Ç—Ä–æ–∫–µ ‚Äî –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π ASCII-–ø—Ä–µ—Ñ–∏–∫—Å
            m_ser_same = re.match(r"\s*([!-~]{4,})", tail_same)
            if m_ser_same:
                return _pack(head_line, m_ser_same.group(1))

            # 2) —Å–µ—Ä–∏–∞–ª –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π/—á–µ—Ä–µ–∑ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫–µ ‚Äî ASCII-–ø—Ä–µ—Ñ–∏–∫—Å —Å—Ç—Ä–æ–∫–∏
            for j in range(i + 1, min(i + 3, len(lines))):
                m_ser_next = RE_ASCII_PREFIX_LINE.match(lines[j])
                if m_ser_next:
                    serial_ascii = m_ser_next.group(1)
                    # –¢—Ä–µ–±—É–µ–º —Ö–æ—Ç—è –±—ã 4 ASCII-—Å–∏–º–≤–æ–ª–∞
                    if len(serial_ascii) >= 4:
                        return _pack(head_line, serial_ascii)
            # –µ—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ ‚Äî —Å–µ—Ä–∏–∞–ª –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ –¥—Ä—É–≥–∏–º —Å—Ç—Ä–æ–∫–∞–º
            # (–Ω–æ —á–∞—â–µ –≤—Å–µ–≥–æ —ç—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)

    # --- –í–ê–†–ò–ê–ù–¢ C: –±–µ–∑ —Å–∫–æ–±–æ–∫ (–∑–∞–≥–æ–ª–æ–≤–æ–∫ '01<14>21' –Ω–∞ —Å—Ç—Ä–æ–∫–µ i + —Å–µ—Ä–∏–∞–ª –Ω–∏–∂–µ) ---
    for i, ln in enumerate(lines):
        if RE_GS1_NOPAREN_HEADLINE.match(ln):
            # —Å–µ—Ä–∏–∞–ª –≤ —ç—Ç–æ–π –∂–µ —Å—Ç—Ä–æ–∫–µ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
            m_same = re.search(r"(?:\(\s*21\s*\)|21)\s*([!-~]{4,})", ln, re.IGNORECASE)
            if m_same:
                head = re.sub(r"\s+", "", ln[:m_same.start(1)])
                tail = re.sub(r"\s+", "", m_same.group(1))
                return head + tail
            # –∏–ª–∏ —Å–µ—Ä–∏–∞–ª –≤ –æ–¥–Ω–æ–π –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫
            for j in range(i + 1, min(i + 3, len(lines))):
                m_next = RE_ASCII_PREFIX_LINE.match(lines[j])
                if m_next and len(m_next.group(1)) >= 4:
                    head = re.sub(r"\s+", "", ln)
                    tail = re.sub(r"\s+", "", m_next.group(1))
                    return head + tail

    # --- Fallback: –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ ---
    return None

def read_pdf(file_path: str | Path) -> str:
    path = Path(file_path)
    parts: list[str] = []
    try:
        with pdfplumber.open(str(path)) as pdf:
            for p in pdf.pages:
                t = p.extract_text()
                if t:
                    parts.append(t.strip())
    except FileNotFoundError:
        print(f"[read_pdf] not found: {path}")
        return ""
    except Exception as e:
        print(f"[read_pdf] failed {path}: {e}")
        return ""
    return "\n".join(parts)


# ---- –ø–æ–∏—Å–∫ PDF –ø–æ (–∞—Ä—Ç–∏–∫—É–ª, —Ä–∞–∑–º–µ—Ä)
def _compile_size_token(size_raw: str) -> re.Pattern:
    """
    –ñ—ë—Å—Ç–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∞ –Ω–µ –ª—é–±–æ–≥–æ).
    - –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–∏—Ä–µ –∫ '-'
    - –¥–æ–ø—É—Å–∫–∞–µ–º '-', '‚Äì', '/', –º–µ–∂–¥—É —á–∏—Å–ª–∞–º–∏
    - –≥—Ä–∞–Ω–∏—Ü—ã —Ç–æ–∫–µ–Ω–∞ (–Ω–µ –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã —Å–ª–µ–≤–∞/—Å–ø—Ä–∞–≤–∞)
    """
    s = re.sub(r"\s+", "", str(size_raw)).upper()
    s = s.replace("‚Äì", "-").replace("‚Äî", "-")
    if re.fullmatch(r"[2-5]?(?:XS|S|M|L|XL|XXL|XXXL)", s):
        return re.compile(rf"(?<![A-Z0-9]){re.escape(s)}(?![A-Z0-9])", re.IGNORECASE | re.MULTILINE)
    token = re.escape(s).replace(r"\-", r"[‚Äì\-\/]")
    return re.compile(rf"(?<!\w){token}(?!\w)", re.IGNORECASE | re.MULTILINE)

def find_pdfs_by_article_size_all(article: str, size: str) -> list[Path]:
    results: list[Path] = []
    if not article or not size:
        return results

    a_no_ws = _strip_all_ws(str(article))
    size_regex = _compile_size_token(size)

    for pdf_file in PDF_DIR.glob("*.pdf"):
        try:
            raw_text = read_pdf(pdf_file)
        except Exception as e:
            print(e)
            continue

        # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–∏—Ä–µ –≤ —Ç–µ–∫—Å—Ç–µ –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–∞
        raw_text_norm = raw_text.replace("‚Äì", "-").replace("‚Äî", "-")

        # —Å—Ç–∞—Ç—å—è –∏—â–µ—Ç—Å—è –ø–æ "—Å–ø–ª—é—â–µ–Ω–Ω–æ–º—É" —Ç–µ–∫—Å—Ç—É (—É—Å—Ç–æ–π—á–∏–≤–æ –∫ –ø–µ—Ä–µ–Ω–æ—Å–∞–º)
        if a_no_ws not in _strip_all_ws(raw_text):
            continue

        # —Ä–∞–∑–º–µ—Ä ‚Äî –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É
        if size_regex.search(raw_text_norm):
            results.append(pdf_file)

    results.sort(key=lambda p: p.name.lower())
    return results

def _build_tail_writer(reader: PdfReader, total: int, keep_indexes: set[int]) -> PdfWriter:
    w = PdfWriter()
    for i in range(total):
        if i in keep_indexes:
            w.add_page(reader.pages[i])
    return w

def _extract_page_code(pl_pdf, page_index: int) -> Optional[str]:
    txt = pl_pdf.pages[page_index].extract_text(x_tolerance=1.0, y_tolerance=1.0) or ""
    return _extract_code_from_text(txt)

async def cut_first_n_pages_unique(session: AsyncSession, src_pdf: Path | str, n: int) -> Tuple[Optional[Path], int]:
    src = Path(src_pdf)
    if n <= 0:
        return None, 0

    tmp_dir = src.parent / "tmp"
    tmp_dir.mkdir(parents=True, exist_ok=True)

    try:
        reader = await _to_thread(PdfReader, str(src))
    except FileNotFoundError:
        print(f"[cut_first_n_pages_unique] not found: {src}")
        return None, n
    except Exception as e:
        print(f"[cut_first_n_pages_unique] PdfReader error for {src}: {e}")
        return None, n

    total_pages = len(reader.pages)
    to_delete: set[int] = set()
    head_writer = PdfWriter()
    unique_taken = 0

    def _read_texts():
        with pdfplumber.open(str(src)) as pl:
            return [pl.pages[i].extract_text(x_tolerance=1.0, y_tolerance=1.0) or "" for i in range(len(pl.pages))]

    try:
        texts = await _to_thread(_read_texts)
    except FileNotFoundError:
        print(f"[cut_first_n_pages_unique] not found while reading: {src}")
        return None, n
    except Exception as e:
        print(f"[cut_first_n_pages_unique] pdfplumber error for {src}: {e}")
        return None, n

    for i in range(total_pages):
        if unique_taken >= n:
            break
        try:
            code = _extract_code_from_text(texts[i])
            if not code:
                continue
            is_new = await register_code_if_new(session, code)
            if is_new:
                head_writer.add_page(reader.pages[i])
                to_delete.add(i)
                unique_taken += 1
            else:
                to_delete.add(i)
        except Exception as e:
            print(f"[cut_first_n_pages_unique] page {i} error: {e}")
            continue

    if unique_taken == 0:
        if to_delete:
            keep = set(range(total_pages)) - to_delete
            tail_writer = _build_tail_writer(reader, total_pages, keep)
            if len(tail_writer.pages) > 0:
                tail_tmp = tmp_dir / f"{src.stem}__tail_tmp.pdf"
                await _to_thread(_write_pdf, tail_writer, tail_tmp)
                await _to_thread(_replace_file, tail_tmp, src)
            else:
                try:
                    await _to_thread(src.unlink, True)
                except Exception as e:
                    print(f"[cut_first_n_pages_unique] unlink error: {e}")
        return None, n

    head_out = tmp_dir / f"{src.stem}__head_{unique_taken}.pdf"
    await _to_thread(_write_pdf, head_writer, head_out)

    keep = set(range(total_pages)) - to_delete
    if keep:
        tail_writer = _build_tail_writer(reader, total_pages, keep)
        tail_tmp = tmp_dir / f"{src.stem}__tail_tmp.pdf"
        await _to_thread(_write_pdf, tail_writer, tail_tmp)
        await _to_thread(_replace_file, tail_tmp, src)
    else:
        try:
            await _to_thread(src.unlink, True)
        except Exception as e:
            print(f"[cut_first_n_pages_unique] unlink error: {e}")

    return head_out, max(0, n - unique_taken)

def merge_pdfs(pdf_paths: list[Path | str], output_path: Path | str) -> Path:
    writer = PdfWriter()
    for p in pdf_paths:
        pth = Path(p)
        if not pth.exists():
            continue
        reader = PdfReader(str(pth))
        for page in reader.pages:
            writer.add_page(page)
    out = Path(output_path)
    out.parent.mkdir(parents=True, exist_ok=True)
    with open(out, "wb") as f:
        writer.write(f)
    return out

def _normalize_columns(df) -> tuple[int, int, int]:
    required = {"–∞—Ä—Ç–∏–∫—É–ª", "—Ä–∞–∑–º–µ—Ä", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ"}
    cols_norm = [str(c).strip().lower() for c in df.columns]
    colset = set(cols_norm)
    if not required.issubset(colset):
        missing = required - colset
        raise ValueError(f"–í df –Ω–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {', '.join(sorted(missing))}")
    return cols_norm.index("–∞—Ä—Ç–∏–∫—É–ª"), cols_norm.index("—Ä–∞–∑–º–µ—Ä"), cols_norm.index("–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ")

def _append_shortage(shortages: list[str], article: str, size: str, amount: int) -> None:
    shortages.append(f"{article} - —Ä–∞–∑–º–µ—Ä: {size}, –Ω–µ —Ö–≤–∞—Ç–∏–ª–æ: {amount}")

async def build_pdf_from_dataframe(df, output_path: Path | str | None = None) -> tuple[Optional[Path], Optional[str]]:
    idx_article, idx_size, idx_qty = _normalize_columns(df)
    cut_parts: list[Path] = []
    shortages: list[str] = []

    async with config.AsyncSessionLocal() as session:
        for _, row in df.iterrows():
            article = str(row.iloc[idx_article]).strip()
            size    = str(row.iloc[idx_size]).strip()
            try:
                qty = int(row.iloc[idx_qty])
            except Exception as e:
                print(e)
                continue
            if qty <= 0:
                continue

            # –æ—Ñ—Ñ–ª–æ–∞–¥ –ø–æ–∏—Å–∫–∞ –ø–æ PDF (–≤–Ω—É—Ç—Ä–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤)
            try:
                pdf_paths = await _to_thread(find_pdfs_by_article_size_all, article, size)
            except Exception as e:
                print(e)
                pdf_paths = []

            if not pdf_paths:
                _append_shortage(shortages, article, size, qty)
                continue

            remaining = qty
            for src_pdf_path in pdf_paths:
                if remaining <= 0: break
                try:
                    part_path, shortage = await cut_first_n_pages_unique(session, src_pdf_path, remaining)
                    took_now = max(0, remaining - shortage)
                    if took_now > 0 and part_path is not None:
                        try:
                            rr = await _to_thread(PdfReader, str(part_path))
                            if len(rr.pages) > 0:
                                cut_parts.append(part_path)
                            else:
                                try:
                                    await _to_thread(Path(part_path).unlink, True)
                                except Exception as e:
                                    print(e)
                        except Exception as e:
                            print(e)
                    remaining -= took_now
                except Exception as e:
                    print(e)
                    pass

            if remaining > 0:
                _append_shortage(shortages, article, size, remaining)

        await session.commit()

    if not cut_parts:
        report = "\n".join(shortages) if shortages else None
        return None, report

    # –æ—Ñ—Ñ–ª–æ–∞–¥ —Å–ª–∏—è–Ω–∏—è PDF
    try:
        result_path = await _to_thread(merge_pdfs, cut_parts, output_path or (PDF_DIR / "result.pdf"))
    except Exception as e:
        print(e)
        result_path = None

    # –æ—Ñ—Ñ–ª–æ–∞–¥ —É–¥–∞–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —á–∞—Å—Ç–µ–π
    for p in cut_parts:
        try:
            await _to_thread(Path(p).unlink, True)
        except Exception as e:
            print(e)

    report = "\n".join(shortages) if shortages else None
    return result_path, report
